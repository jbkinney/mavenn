{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Tutorial 2: Protein DMS modeling using additive G-P maps\n",
    "\n",
    "This tutorial covers perhaps the simplest application of MAVE-NN: the modeling of DMS data using an additive genotype-phenotype (G-P) map together with a global epistasis (GE) measurement process. The code below steps users through this process, and can be used to train models similar to the following built-in models, which are accessible using `mavenn.load_example_model()`:\n",
    "\n",
    "- `'amyloid_additive_ge'`\n",
    "- `'tdp43_additive_ge'`\n",
    "- `gb1_additive_ge`:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-14T13:44:54.463235Z",
     "start_time": "2021-11-14T13:44:52.073819Z"
    }
   },
   "outputs": [],
   "source": [
    "# Standard imports\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Import MAVE-NN\n",
    "import mavenn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "First we choose which dataset we wish to model, and we load it as a Pandas dataframe using `mavenn.load_example_dataset()`. We then compute the length of sequences in that dataset; we will need this quantity for defining the architecture of our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-14T13:54:20.715368Z",
     "start_time": "2021-11-14T13:54:20.659067Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset 'amyloid' \n",
      "Sequence length: 42 amino acids (+ stops)\n",
      "data_df:\n"
     ]
    },
    {
     "data": {
      "text/plain": "            set  dist         y        dy  \\\n0      training     1 -0.117352  0.387033   \n1      training     1  0.352500  0.062247   \n2      training     1 -2.818013  1.068137   \n3      training     1  0.121805  0.376764   \n4      training     1 -2.404340  0.278486   \n...         ...   ...       ...       ...   \n16061  training     2 -0.151502  0.389821   \n16062  training     2 -1.360708  0.370517   \n16063  training     2 -0.996816  0.346949   \n16064  training     2 -3.238403  0.429008   \n16065  training     2 -1.141457  0.365638   \n\n                                                x  \n0      KAEFRHDSGYEVHHQKLVFFAEDVGSNKGAIIGLMVGGVVIA  \n1      NAEFRHDSGYEVHHQKLVFFAEDVGSNKGAIIGLMVGGVVIA  \n2      TAEFRHDSGYEVHHQKLVFFAEDVGSNKGAIIGLMVGGVVIA  \n3      SAEFRHDSGYEVHHQKLVFFAEDVGSNKGAIIGLMVGGVVIA  \n4      IAEFRHDSGYEVHHQKLVFFAEDVGSNKGAIIGLMVGGVVIA  \n...                                           ...  \n16061  DAEFRHDSGYEVHHQKLVFFAEDVGSNKGAIIGLMVGGVVKV  \n16062  DAEFRHDSGYEVHHQKLVFFAEDVGSNKGAIIGLMVGGVVLV  \n16063  DAEFRHDSGYEVHHQKLVFFAEDVGSNKGAIIGLMVGGVVMV  \n16064  DAEFRHDSGYEVHHQKLVFFAEDVGSNKGAIIGLMVGGVVTV  \n16065  DAEFRHDSGYEVHHQKLVFFAEDVGSNKGAIIGLMVGGVVVV  \n\n[16066 rows x 5 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>set</th>\n      <th>dist</th>\n      <th>y</th>\n      <th>dy</th>\n      <th>x</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>training</td>\n      <td>1</td>\n      <td>-0.117352</td>\n      <td>0.387033</td>\n      <td>KAEFRHDSGYEVHHQKLVFFAEDVGSNKGAIIGLMVGGVVIA</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>training</td>\n      <td>1</td>\n      <td>0.352500</td>\n      <td>0.062247</td>\n      <td>NAEFRHDSGYEVHHQKLVFFAEDVGSNKGAIIGLMVGGVVIA</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>training</td>\n      <td>1</td>\n      <td>-2.818013</td>\n      <td>1.068137</td>\n      <td>TAEFRHDSGYEVHHQKLVFFAEDVGSNKGAIIGLMVGGVVIA</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>training</td>\n      <td>1</td>\n      <td>0.121805</td>\n      <td>0.376764</td>\n      <td>SAEFRHDSGYEVHHQKLVFFAEDVGSNKGAIIGLMVGGVVIA</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>training</td>\n      <td>1</td>\n      <td>-2.404340</td>\n      <td>0.278486</td>\n      <td>IAEFRHDSGYEVHHQKLVFFAEDVGSNKGAIIGLMVGGVVIA</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>16061</th>\n      <td>training</td>\n      <td>2</td>\n      <td>-0.151502</td>\n      <td>0.389821</td>\n      <td>DAEFRHDSGYEVHHQKLVFFAEDVGSNKGAIIGLMVGGVVKV</td>\n    </tr>\n    <tr>\n      <th>16062</th>\n      <td>training</td>\n      <td>2</td>\n      <td>-1.360708</td>\n      <td>0.370517</td>\n      <td>DAEFRHDSGYEVHHQKLVFFAEDVGSNKGAIIGLMVGGVVLV</td>\n    </tr>\n    <tr>\n      <th>16063</th>\n      <td>training</td>\n      <td>2</td>\n      <td>-0.996816</td>\n      <td>0.346949</td>\n      <td>DAEFRHDSGYEVHHQKLVFFAEDVGSNKGAIIGLMVGGVVMV</td>\n    </tr>\n    <tr>\n      <th>16064</th>\n      <td>training</td>\n      <td>2</td>\n      <td>-3.238403</td>\n      <td>0.429008</td>\n      <td>DAEFRHDSGYEVHHQKLVFFAEDVGSNKGAIIGLMVGGVVTV</td>\n    </tr>\n    <tr>\n      <th>16065</th>\n      <td>training</td>\n      <td>2</td>\n      <td>-1.141457</td>\n      <td>0.365638</td>\n      <td>DAEFRHDSGYEVHHQKLVFFAEDVGSNKGAIIGLMVGGVVVV</td>\n    </tr>\n  </tbody>\n</table>\n<p>16066 rows × 5 columns</p>\n</div>"
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Choose dataset\n",
    "data_name = 'amyloid'\n",
    "# data_name = 'tdp43'\n",
    "# data_name = 'gb1'\n",
    "print(f\"Loading dataset '{data_name}' \")\n",
    "\n",
    "# Load datset\n",
    "data_df = mavenn.load_example_dataset(data_name)\n",
    "\n",
    "# Get and report sequence length\n",
    "L = len(data_df.loc[0,'x'])\n",
    "print(f'Sequence length: {L:d} amino acids (+ stops)')\n",
    "\n",
    "# Preview dataset\n",
    "print('data_df:')\n",
    "data_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we use the built-in function `mavenn.split_dataset()` to split our dataset into two dataframes:\n",
    "\n",
    "- `trainval_df`, which contains both the training set and validation set data.\n",
    "- `test_df`, which contains only the test data.\n",
    "\n",
    "This split ensures that our test data stays separate from our training and validation data. Note that `training_df`, previewed below, includes an extra column called `'validation'` that flags which sequences are reserved for the validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-14T13:55:49.662729Z",
     "start_time": "2021-11-14T13:55:49.634762Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set   :   14,481 observations (  90.13%)\n",
      "Validation set :      826 observations (   5.14%)\n",
      "Test set       :      759 observations (   4.72%)\n",
      "-------------------------------------------------\n",
      "Total dataset  :   16,066 observations ( 100.00%)\n",
      "\n",
      "trainval_df:\n"
     ]
    },
    {
     "data": {
      "text/plain": "       validation  dist         y        dy  \\\n0           False     1 -0.117352  0.387033   \n1           False     1  0.352500  0.062247   \n2           False     1 -2.818013  1.068137   \n3           False     1  0.121805  0.376764   \n4           False     1 -2.404340  0.278486   \n...           ...   ...       ...       ...   \n15302       False     2 -0.151502  0.389821   \n15303       False     2 -1.360708  0.370517   \n15304       False     2 -0.996816  0.346949   \n15305       False     2 -3.238403  0.429008   \n15306       False     2 -1.141457  0.365638   \n\n                                                x  \n0      KAEFRHDSGYEVHHQKLVFFAEDVGSNKGAIIGLMVGGVVIA  \n1      NAEFRHDSGYEVHHQKLVFFAEDVGSNKGAIIGLMVGGVVIA  \n2      TAEFRHDSGYEVHHQKLVFFAEDVGSNKGAIIGLMVGGVVIA  \n3      SAEFRHDSGYEVHHQKLVFFAEDVGSNKGAIIGLMVGGVVIA  \n4      IAEFRHDSGYEVHHQKLVFFAEDVGSNKGAIIGLMVGGVVIA  \n...                                           ...  \n15302  DAEFRHDSGYEVHHQKLVFFAEDVGSNKGAIIGLMVGGVVKV  \n15303  DAEFRHDSGYEVHHQKLVFFAEDVGSNKGAIIGLMVGGVVLV  \n15304  DAEFRHDSGYEVHHQKLVFFAEDVGSNKGAIIGLMVGGVVMV  \n15305  DAEFRHDSGYEVHHQKLVFFAEDVGSNKGAIIGLMVGGVVTV  \n15306  DAEFRHDSGYEVHHQKLVFFAEDVGSNKGAIIGLMVGGVVVV  \n\n[15307 rows x 5 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>validation</th>\n      <th>dist</th>\n      <th>y</th>\n      <th>dy</th>\n      <th>x</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>False</td>\n      <td>1</td>\n      <td>-0.117352</td>\n      <td>0.387033</td>\n      <td>KAEFRHDSGYEVHHQKLVFFAEDVGSNKGAIIGLMVGGVVIA</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>False</td>\n      <td>1</td>\n      <td>0.352500</td>\n      <td>0.062247</td>\n      <td>NAEFRHDSGYEVHHQKLVFFAEDVGSNKGAIIGLMVGGVVIA</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>False</td>\n      <td>1</td>\n      <td>-2.818013</td>\n      <td>1.068137</td>\n      <td>TAEFRHDSGYEVHHQKLVFFAEDVGSNKGAIIGLMVGGVVIA</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>False</td>\n      <td>1</td>\n      <td>0.121805</td>\n      <td>0.376764</td>\n      <td>SAEFRHDSGYEVHHQKLVFFAEDVGSNKGAIIGLMVGGVVIA</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>False</td>\n      <td>1</td>\n      <td>-2.404340</td>\n      <td>0.278486</td>\n      <td>IAEFRHDSGYEVHHQKLVFFAEDVGSNKGAIIGLMVGGVVIA</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>15302</th>\n      <td>False</td>\n      <td>2</td>\n      <td>-0.151502</td>\n      <td>0.389821</td>\n      <td>DAEFRHDSGYEVHHQKLVFFAEDVGSNKGAIIGLMVGGVVKV</td>\n    </tr>\n    <tr>\n      <th>15303</th>\n      <td>False</td>\n      <td>2</td>\n      <td>-1.360708</td>\n      <td>0.370517</td>\n      <td>DAEFRHDSGYEVHHQKLVFFAEDVGSNKGAIIGLMVGGVVLV</td>\n    </tr>\n    <tr>\n      <th>15304</th>\n      <td>False</td>\n      <td>2</td>\n      <td>-0.996816</td>\n      <td>0.346949</td>\n      <td>DAEFRHDSGYEVHHQKLVFFAEDVGSNKGAIIGLMVGGVVMV</td>\n    </tr>\n    <tr>\n      <th>15305</th>\n      <td>False</td>\n      <td>2</td>\n      <td>-3.238403</td>\n      <td>0.429008</td>\n      <td>DAEFRHDSGYEVHHQKLVFFAEDVGSNKGAIIGLMVGGVVTV</td>\n    </tr>\n    <tr>\n      <th>15306</th>\n      <td>False</td>\n      <td>2</td>\n      <td>-1.141457</td>\n      <td>0.365638</td>\n      <td>DAEFRHDSGYEVHHQKLVFFAEDVGSNKGAIIGLMVGGVVVV</td>\n    </tr>\n  </tbody>\n</table>\n<p>15307 rows × 5 columns</p>\n</div>"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Split dataset \n",
    "trainval_df, test_df = mavenn.split_dataset(data_df)\n",
    "\n",
    "# Preview trainval_df\n",
    "print('trainval_df:')\n",
    "trainval_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we specify the architecture of the model we wish to train . To do this, we create an instance of the `mavenn.Model` class, called `model`, using the following keyword arguments:\n",
    "\n",
    "- `L=L` specifies the sequence length.\n",
    "- `alphabet='protein*'` specifies that the alphabet our sequences are built from consists of 21 characters representing the 20 amino acids plus a stop signal (which is represented by the character `'*'`). Other possible choices for `alphabet` are `'dna'`, `'rna'`, and `'protein'`.\n",
    "- `gpmap_type='additive'` specifies that we wish to infer an additive G-P map.\n",
    "- `regression_type='GE'` specifies that our model will have a global epistasis (GE) measurement process. We choose this because our MAVE measurements are continuous real numbers.\n",
    "- `ge_noise_model_type='SkewedT'` specifies the use of a skewed-t noise model in the GE measurement process. The `'SkewedT'` noise model can accommodate asymmetric noise and is thus more flexible than the default `'Gaussian'` noise model.\n",
    "- `ge_heteroskedasticity_order=2` specifies that the noise model parameters (the three parameters of the skewed-t distribution) are each modeled using quadratic functions of the predicted measurement $\\hat{y}$. This will allow our model of experimental noise to vary with signal intensity.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then set the training data by calling `model.set_data()`. The keyword argument `'validation_flags'` is used to specify which subset of the data in `trainval_df` will be used for validation (as opposed to stochastic gradient descent)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-12T23:00:20.975604Z",
     "start_time": "2021-11-12T23:00:20.973737Z"
    }
   },
   "source": [
    "Next we train the model by calling `model.fit()`. In doing so we specify a number of hyperparameters including the learning rate, the number of epochs, the batch size, whether to use early stopping, and the early stopping patience.  We also set `verbose=False` to limit the amount of user feedback.\n",
    "\n",
    "Choosing hyperparameters is somewhat of an art, and the particular values used here were found by trial and error. In general users will have to try a number of different values for these and possibly other hyperparameters in order to find ones that work well. We recommend that users choose these hyperparameters in order to maximize the final value for `val_I_var`, the variational information of the trained model on the validation dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-14T14:22:03.896770Z",
     "start_time": "2021-11-14T14:21:33.523942Z"
    }
   },
   "outputs": [],
   "source": [
    "# Define model\n",
    "model = mavenn.Model(L=L,\n",
    "                     alphabet='protein*',\n",
    "                     gpmap_type='additive',   \n",
    "                     regression_type='GE',\n",
    "                     ge_noise_model_type='SkewedT',\n",
    "                     ge_heteroskedasticity_order=2)\n",
    "\n",
    "# Set training data\n",
    "model.set_data(x=trainval_df['x'],\n",
    "               y=trainval_df['y'],\n",
    "               validation_flags=trainval_df['validation'])\n",
    "\n",
    "# Train model\n",
    "model.fit(learning_rate=1e-3,\n",
    "          epochs=500,\n",
    "          batch_size=64,\n",
    "          early_stopping=True,\n",
    "          early_stopping_patience=25,\n",
    "          verbose=False);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To assess the performance of our final trained model, we compute two different metrics on test data: **variational information** and **predictive information**. Variational information quantifies the performance of the full latent phenotype model, whereas predictive information quantifies the performance of just the G-P map portion of the model. See Tareen et al. (2021) for an expanded discussion of these quantities.\n",
    "\n",
    "Note that MAVE-NN also estimates the standard errors for these quantities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-14T14:26:26.796040Z",
     "start_time": "2021-11-14T14:26:26.707225Z"
    }
   },
   "outputs": [],
   "source": [
    "# Compute variational information on test data\n",
    "I_var, dI_var =  model.I_variational(x=test_df['x'], y=test_df['y'])\n",
    "print(f'test_I_var: {I_var:.3f} +- {dI_var:.3f} bits')\n",
    "\n",
    "# Compute predictive information on test data\n",
    "I_pred, dI_pred = model.I_predictive(x=test_df['x'], y=test_df['y'])\n",
    "print(f'test_I_pred: {I_pred:.3f} +- {dI_pred:.3f} bits')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To save the trained model we call `model.save()`. This records our model in **two separate files**: a pickle file that defines model architecture (extension `'.pickle'`), and an H5 file that records model parameters (extension `'.h5'`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-14T14:26:29.737382Z",
     "start_time": "2021-11-14T14:26:29.719622Z"
    }
   },
   "outputs": [],
   "source": [
    "# Save model to file\n",
    "model_name = f'{data_name}_additive_ge'\n",
    "model.save(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization\n",
    "\n",
    "We now discuss how to visualize the training history, performance, and parameters of a trained model. First we  then load our model using `mavenn.load`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-14T14:26:31.729396Z",
     "start_time": "2021-11-14T14:26:31.641758Z"
    }
   },
   "outputs": [],
   "source": [
    "# Delete model if it is present in memory\n",
    "try:\n",
    "    del model\n",
    "except:\n",
    "    pass\n",
    "\n",
    "# Load model from file\n",
    "model = mavenn.load(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `model.history` dictionary contains values for multiple model performance metrics, evaluated on both the training data and the validation data, as a function of training epoch. `'loss'` and `'val_loss'` record loss values, while `'I_var'` and `'val_I_var'` record the variational information values. As described in Tareen et al. (2021), these metrics are closely related: variational information is an affine transformation of model likelihood, whereas loss is equal to likelihood plus regularization terms. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-14T14:26:32.530670Z",
     "start_time": "2021-11-14T14:26:32.526494Z"
    }
   },
   "outputs": [],
   "source": [
    "# Show metrics recorded in model.history()\n",
    "model.history.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plotting `'I_var'` and `'val_I_var'` versus epoch can often provide insight into the model training process. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-14T14:26:33.704972Z",
     "start_time": "2021-11-14T14:26:33.599070Z"
    }
   },
   "outputs": [],
   "source": [
    "# Create figure and axes for plotting\n",
    "fig, ax = plt.subplots(1,1,figsize=[5,5])\n",
    "\n",
    "# Plot I_var_train, the variational information on training data as a function of epoch\n",
    "ax.plot(model.history['I_var'], \n",
    "        label=r'I_var_train')\n",
    "\n",
    "# Plot I_var_val, the variational information on validation data as a function of epoch\n",
    "ax.plot(model.history['val_I_var'], \n",
    "        label=r'val_I_var')\n",
    "\n",
    "# Show I_var_test, the variational information of the final model on test data\n",
    "ax.axhline(I_var, color='C2', linestyle=':', \n",
    "           label=r'test_I_var')\n",
    "\n",
    "# Show I_pred_test, the predictive information of the final model on test data\n",
    "ax.axhline(I_pred, color='C3', linestyle=':', \n",
    "           label=r'test_I_pred')\n",
    "\n",
    "# Style plot\n",
    "ax.set_xlabel('epochs')\n",
    "ax.set_ylabel('bits')\n",
    "ax.set_title('Training history: variational information')\n",
    "ax.set_ylim([0, 1.2*I_pred])\n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From this plot we observe a few things:\n",
    "\n",
    "- `'val_I_var'` quickly converges to its final value, suggesting that model training has likely gone to completion.\n",
    "- `'I_var'` is noticably higher than `'val_I_var'`, which is indicative of overfitting. But these two quantities largely remain monotonically related to one another, indicating that this overfitting is likely begnin. \n",
    "- `'test_I_var'` is quite close to `'test_I_pred'` indicating that the inferred measurement process does a good job of describing the scatter of $y$ about $\\hat{y}$.\n",
    "\n",
    "Users can plot '`loss`' and '`var_loss`' if they like, though the absolute values these quantities are be more difficult to interpret than `'I_var'` and `'val_I_var'`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-14T14:26:46.966898Z",
     "start_time": "2021-11-14T14:26:46.871063Z"
    }
   },
   "outputs": [],
   "source": [
    "# Create figure and axes for plotting\n",
    "fig, ax = plt.subplots(1,1,figsize=[5,5])\n",
    "\n",
    "# Plot loss_train, the loss computed on training data as a function of epoch\n",
    "ax.plot(model.history['loss'], \n",
    "        label=r'loss_train')\n",
    "\n",
    "# Plot loss_val, the loss computed on validation data as a function of epoch\n",
    "ax.plot(model.history['val_loss'], \n",
    "        label=r'val_I_var')\n",
    "\n",
    "# Style plot\n",
    "ax.set_xlabel('epochs')\n",
    "ax.set_ylabel('loss')\n",
    "ax.set_title('Training hisotry: loss')\n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is also useful to consider more traditional metrics of model performance. In the context of GE models, a natural choice is $R^2$ between measurements $y$ and model predictions $\\hat{y}$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-14T14:27:00.877140Z",
     "start_time": "2021-11-14T14:27:00.740057Z"
    }
   },
   "outputs": [],
   "source": [
    "# Create figure and axes for plotting\n",
    "fig, ax = plt.subplots(1,1,figsize=[5,5])\n",
    "\n",
    "# Get test data y values\n",
    "y_test = test_df['y']\n",
    "\n",
    "# Compute yhat on test data\n",
    "yhat_test = model.x_to_yhat(test_df['x'])\n",
    "\n",
    "# Compute R^2 between yhat_test and y_test\n",
    "Rsq = np.corrcoef(yhat_test.ravel(), test_df['y'])[0, 1]**2\n",
    "\n",
    "# Plot y_test vs. yhat_test\n",
    "ax.scatter(yhat_test, y_test, color='C0', s=10, alpha=.3, \n",
    "           label='test data')\n",
    "\n",
    "# Style plot\n",
    "xlim = [min(yhat_test), max(yhat_test)]\n",
    "ax.plot(xlim, xlim, '--', color='k', label='diagonal', zorder=100)\n",
    "ax.set_xlabel('model prediction ($\\hat{y}$)')\n",
    "ax.set_ylabel('measurement ($y$)')\n",
    "ax.set_title(f'Standard metric of model performance:\\n$R^2$={Rsq:.3}');\n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we visualize the GE measurement process inferred as part of our latent phenotype model. Recall from Tareen et al. (2021) that the measurement process consists of \n",
    "  \n",
    "- A nonlinearity $\\hat{y} = g(\\phi)$ that deterministically maps the latent phenotype $\\phi$ to a prediction $\\hat{y}$.\n",
    "- A noise model $p(y|\\hat{y})$ that stochastically maps predictions $\\hat{y}$ to measurements $y$.\n",
    "\n",
    "We can conveniently visualize both of these quantities in a single \"global epistatsis plot\": "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-14T14:39:47.462070Z",
     "start_time": "2021-11-14T14:39:47.367681Z"
    }
   },
   "outputs": [],
   "source": [
    "# Create figure and axes for plotting\n",
    "fig, ax = plt.subplots(1,1,figsize=[5,5])\n",
    "\n",
    "# Get test data y values\n",
    "y_test = test_df['y']\n",
    "\n",
    "# Compute φ on test data\n",
    "phi_test = model.x_to_phi(test_df['x'])\n",
    "\n",
    "## Set phi lims and create a grid in phi space\n",
    "phi_lim = [min(phi_test)-.5, max(phi_test)+.5]\n",
    "phi_grid = np.linspace(phi_lim[0], phi_lim[1], 1000)\n",
    "\n",
    "# Compute yhat each phi gridpoint\n",
    "yhat_grid = model.phi_to_yhat(phi_grid)\n",
    "\n",
    "# Compute 95% CI for each yhat\n",
    "q = [0.025, 0.975]\n",
    "yqs_grid = model.yhat_to_yq(yhat_grid, q=q)\n",
    "\n",
    "\n",
    "# Plote 95% confidence interval\n",
    "ax.fill_between(phi_grid, yqs_grid[:, 0], yqs_grid[:, 1], \n",
    "                alpha=0.2, color='C1', lw=0, label='95% CI')\n",
    "\n",
    "# Plot GE nonlinearity\n",
    "ax.plot(phi_grid, yhat_grid, \n",
    "        linewidth=3, color='C1', label='nonlinearity')\n",
    "\n",
    "# Plot scatter of φ and y values. \n",
    "ax.scatter(phi_test, y_test, \n",
    "           color='C0', s=10, alpha=.3, label='test data', zorder=+100)\n",
    "\n",
    "# Style plot\n",
    "ax.set_xlim(phi_lim)\n",
    "ax.set_xlabel('latent phenotype ($\\phi$)')\n",
    "ax.set_ylabel('measurement ($y$)')\n",
    "ax.set_title('GE measurement process')\n",
    "ax.legend()\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To retrieve the values of our model's G-P map parameters, we use the mmethod `model.get_theta()`. This returns a dictionary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-14T15:01:30.220714Z",
     "start_time": "2021-11-14T15:01:30.179189Z"
    }
   },
   "outputs": [],
   "source": [
    "# Retrieve G-P map parameter dict and view dict keys\n",
    "theta_dict = model.get_theta(gauge='consensus')\n",
    "theta_dict.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is important to appreciate that G-P maps usually have many non-identifiable directions in parameter space. These are called \"gauge freedoms\".  Interpreting the values of model parameters requires that we first \"pin down\" these gauge freedoms by using a clearly specified convension. Specifying `gauge='consensus'` in `model.get_theta()` accomplishes this fixing all the $\\theta_{l:c}$ parameters that contribute to the consensus sequence to zero. This convension allows all the other $\\theta_{l:c}$ parameters in the additive model to be interpreted as single-mutation effects $\\Delta \\phi$ away from the consensus sequence. \n",
    "\n",
    "Finally, we use `mavenn.heatmap()` to visualize these additive parameters. This function takes a number of keyword arguments, which we summarize here. More information can be found in this function's docstring. \n",
    "\n",
    "- `ax=ax`: specifies the axes on which to draw both the heatmap and the colorbar. \n",
    "- `values=theta_dict['theta_lc']`: specifies the additive parameters in the form of a `np.array` of size `L`x`C`, where `C` is the alphabet size. \n",
    "- `alphabet=theta_dict['alphabet']`: provides a list of characters corresponding to the columns of `values`. \n",
    "- `seq=model.x_stats['consensus_seq']`: causes `mavenn.heatmap()` to highlight the characters of a specific sequence of interest. In our case this is the consensus sequence, the additive parameters for which are all fixed to zero. \n",
    "- `seq_kwargs={'c':'gray', 's':25}`: provides a keyword dictionary to pass to `ax.scatter()`; this specifies how the characters of the sequence of interest are to be graphically indicated. \n",
    "- `cmap='coolwarm'`: specifies the colormap used to represent the values of the additive parameters.\n",
    "- `cbar=True`: specifies that a colorbar be drawn\n",
    "- `cmap_size='2%'`: specifies the width of the colorbar relative to the enclosing ax object.\n",
    "- `cmap_pad=.3`: specifies the spacing between the heatmap and the colorbar\n",
    "- `ccenter=0`: centers the colormap at zero. \n",
    "\n",
    "This function returns two objects: \n",
    "- `heatmap_ax` is the axes object on which the heatmap is drawn.\n",
    "- `cb` is the colorbar object; it's corresponding axes is given by `cb.ax`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-14T15:52:30.805195Z",
     "start_time": "2021-11-14T15:52:30.568250Z"
    }
   },
   "outputs": [],
   "source": [
    "# Create figure\n",
    "fig, ax = plt.subplots(1,1, figsize=(12,5))\n",
    "\n",
    "# Draw heatmap\n",
    "heatmap_ax, cb = mavenn.heatmap(ax=ax,\n",
    "                                values=theta_dict['theta_lc'],\n",
    "                                alphabet=theta_dict['alphabet'],\n",
    "                                seq=model.x_stats['consensus_seq'],\n",
    "                                seq_kwargs={'c':'gray', 's':25},\n",
    "                                cmap='coolwarm',\n",
    "                                cbar=True,\n",
    "                                cmap_size='2%',\n",
    "                                cmap_pad=.3,\n",
    "                                ccenter=0)\n",
    "# Style heatmap (can be different between two dataset)\n",
    "#heatmap_ax.set_xticks()\n",
    "heatmap_ax.tick_params(axis='y', which='major', pad=10)\n",
    "heatmap_ax.set_xlabel('position ($l$)')\n",
    "heatmap_ax.set_ylabel('amino acid ($c$)')\n",
    "heatmap_ax.set_title(f'Additive parameters: {model_name}')\n",
    "\n",
    "# Style colorbar\n",
    "cb.outline.set_visible(False)\n",
    "cb.ax.tick_params(direction='in', size=20, color='white')\n",
    "cb.set_label('mutational effect ($\\Delta \\phi$)', labelpad=5, rotation=-90, ha='center', va='center')\n",
    "\n",
    "# Adjust figure and show\n",
    "fig.tight_layout(w_pad=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that many of the squares in the heatmap are white. These correspond to additive parameters whose values are `NaN`. MAVE-NN sets the values of a feature effect to `NaN` when no variant in the training set exhibits that feature. Such `NaN` parameters are common, even among additive parameters, as DMS libraries often do not contain a comprehensive set of single-amino-acid mutations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "1. Tareen, A., Posfai, A., Ireland, W. T., McCandlish, D. M. & Kinney, J. B. MAVE-NN: learning genotype-phenotype maps from multiplex assays of variant effect. bioRxiv doi:10.1101/2020.07.14.201475 (2020).\n",
    "1. Seuma, M., Faure, A., Badia, M., Lehner, B. & Bolognesi, B. The genetic landscape for amyloid beta fibril nucleation accurately discriminates familial Alzheimer’s disease mutations. eLife 10, e63364 (2021).\n",
    "  \n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}